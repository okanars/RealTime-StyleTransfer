Real-Time Video Style Transfer (Edge-Optimized Implementation)A lightweight, PyTorch-based implementation of Fast Neural Style Transfer (based on Johnson et al., 2016), engineered to perform real-time artistic stylization on video streams and webcam feeds using resource-constrained hardware.(Above: Real-time inference output demonstrating artistic style transfer. Left: Input, Right: Stylized Output)üìã Project OverviewThis project implements a feed-forward Convolutional Neural Network (CNN) capable of applying artistic styles to input images/video with low latency. Unlike optimization-based methods (Gatys et al.) which take minutes per image, or heavy Diffusion models requiring massive VRAM, this architecture separates training and inference, allowing for real-time processing suitable for video applications.Key Features:Real-Time Inference: Optimized for webcam (30+ FPS) and video processing.Architecture: Encoder-Bottleneck-Decoder with Residual Blocks and Instance Normalization.Hardware Agnostic: Supports CUDA (NVIDIA), MPS (Apple Silicon), and CPU execution.‚ö†Ô∏è Engineering Constraints & Optimization StrategyThis implementation was designed as a Proof of Concept (PoC) to demonstrate feasibility on hardware with strict memory bandwidth and VRAM limitations (e.g., Local Mac/PC without enterprise GPUs).To achieve successful model convergence within these constraints, the following engineering trade-offs were applied:1. Resource ManagementBatch Size: Restricted to 2 to prevent Out-Of-Memory (OOM) errors during the backward pass.Resolution: Training performed at 256x256 to maintain a manageable tensor footprint while preserving structural features.2. Training Efficiency & DataDataset Strategy: Utilized a curated subset of high-quality images rather than the full multi-gigabyte COCO dataset. This proves the model's ability to learn semantic content features without requiring terabytes of storage.Epoch Optimization: Training limited to 2 epochs. While extended training would refine high-frequency details, this duration was sufficient to validate the loss function convergence and style transfer capability.Despite these aggressive constraints, the model successfully generalizes the style textures and preserves content structure, validating the architectural approach.üèó Technical Architecture1. The Generator (TransformerNet)A lightweight CNN that transforms the input image.Downsampling: Convolutional layers to compress spatial dimensions.Bottleneck: 5 Residual Blocks to learn complex features.Upsampling: Uses Upsample + Conv2d instead of ConvTranspose2d to eliminate checkerboard artifacts in the final output.Normalization: Uses Instance Normalization instead of Batch Normalization, as it has been proven to yield better results for stylization tasks.2. The Loss Network (VGG16)A pre-trained VGG16 model (frozen) is used solely during training to compute Perceptual Loss:Content Loss: Computed at layer relu2_2.Style Loss: Computed via Gram Matrices at layers relu1_2, relu2_2, relu3_3, and relu4_3.üìä Dataset CreditsThe content images used for training were sourced from the Unsplash Random Images Collection on Kaggle.Source: Unsplash Random Images CollectionLicense: Public Domain / Unsplash License.‚öôÔ∏è Setup & InstallationClone the repository:git clone [https://github.com/yourusername/RealTimeTransfer.git](https://github.com/yourusername/RealTimeTransfer.git)
cd RealTimeTransfer
Install dependencies:pip install -r requirements.txt
Prepare Data:Place your content images (jpg/png) in data/content/.Place your style images (e.g., paintings) in data/style/.üöÄ Usage1. TrainingTo train a new model on a specific style (e.g., Cyberpunk or Van Gogh):# Train on a style image located at data/style/style.jpg
python train.py \
  --content-dir ./data/content \
  --style-image ./data/style/style.jpg \
  --batch-size 2 \
  --epochs 2 \
  --save-model-dir ./checkpoints
Note: Mac users (M1/M2/M3) will automatically utilize MPS (Metal Performance Shaders) acceleration.2. Inference (Webcam Demo)Run the trained model on your live webcam feed.Note: Adjust --image-size to 480 or 360 for higher FPS on non-GPU machines.python inference.py --model checkpoints/model.pth --webcam --image-size 480
3. Video ProcessingApply style to an existing video file:python inference.py \
  --model checkpoints/model.pth \
  --input my_video.mp4 \
  --output stylized_video.mp4
üìà Future Improvements (Roadmap)With access to higher-capacity compute resources, the following improvements are planned:Increase Batch Size to 8-16 for more stable gradient descent.Train on the full COCO 2017 dataset for better generalization.Implement Temporal Consistency loss (Optical Flow) to reduce flickering in videos.Export model to ONNX for web/mobile deployment.